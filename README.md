# nietzsche-gpt
ğŸ§  Nietzsche-GPT

A miniature GPT language model trained from scratch on Nietzscheâ€™s writings.

This project implements a decoder-only Transformer entirely from scratch in PyTorch, trained on a Nietzsche text dataset (~1MB).
The model learns to predict the next character using self-attention, and generates original text in a Nietzsche-like style.

ğŸš€ Features

Full Transformer architecture implemented manually

Multi-Head Self-Attention (MHSA)

Causal masking for autoregressive generation

Feedforward MLP layers

Residual connections + LayerNorm

Learned token + positional embeddings

Character-level tokenizer

Training pipeline from scratch

Interactive CLI for chatting with NietzscheGPT

ğŸ“š Methodology
1ï¸âƒ£ Dataset

Simple UTF-8 .txt file (nietzsche.txt)

Character-level vocabulary (each unique character â†’ token)

We build:

stoi (char â†’ id)

itos (id â†’ char)

encode(string)

decode(list[int])

The full dataset is encoded to integers and split:

90% training

10% validation

2ï¸âƒ£ Objective: Next-Token Prediction

The model is trained to:

Predict the next character given previous characters.

This forces the model to learn:

grammar

structure

dependencies

Nietzscheâ€™s writing style

We use:

Cross-entropy loss

AdamW optimizer

3ï¸âƒ£ Model Architecture
ğŸ”· miniGPT (GPT-style Transformer)
Token Embeddings
+ Position Embeddings
â†“
[ Transformer Block Ã— N ]
â†“
LayerNorm
â†“
Linear head â†’ vocabulary logits

4ï¸âƒ£ Self-Attention (Core Mechanism)

Each token produces vectors:

Query Q

Key K

Value V

Attention:

A = softmax( Q Â· Káµ€ / sqrt(d_k) )
Output = A @ V

â›” Causal Mask

The model cannot attend to future tokens.
We use torch.tril() to enforce autoregressive behavior.

5ï¸âƒ£ Multi-Head Attention (MHA)

Multiple attention heads:

capture different relationships

concatenate results

project back to embedding dimension

6ï¸âƒ£ Feedforward Neural Network (MLP)

Each token independently passes through:

Linear â†’ GELU â†’ Linear


Expanded width (4Ã— embedding size) adds non-linearity and reasoning ability.

7ï¸âƒ£ Residual Connections + LayerNorm

We use the GPT-2 pre-norm architecture:

x = x + MHA( LayerNorm(x) )
x = x + MLP( LayerNorm(x) )


Benefits:

stable training

deeper networks train effectively

8ï¸âƒ£ Training Procedure

Training loop:

Sample random sequences (block_size)

Predict the next token

Compute loss

Backpropagate

Update weights

Validation loss is logged periodically to monitor learning.

9ï¸âƒ£ Text Generation (Sampling)

Generation uses:

Last-step logits

Softmax to get probabilities

torch.multinomial to sample next character

Append token â†’ repeat

Example output:

what is truth, feelings; the false in the most great
nature.
Our books Her last of trausifically to them ext...

ğŸ’¬ Interactive Mode

Use interact.py to chat with NietzscheGPT:

You: what is truth?
NietzscheGPT: what is truth, feelings; the false in the most great nature...

ğŸ›  Project Structure
nietzsche-gpt/
â”‚
â”œâ”€â”€ data.py               # dataset, vocab, encode/decode
â”œâ”€â”€ model.py              # full Transformer implementation
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py          # training loop
â”‚   â”œâ”€â”€ interact.py       # interactive CLI chat
â”‚
â”œâ”€â”€ nietzsche.txt         # dataset
â””â”€â”€ README.md             # this file

ğŸ“ˆ Results

Learns recognizable Nietzsche-like structure

Semi-coherent philosophical phrases

Val loss improves steadily

Can be scaled easily for better output